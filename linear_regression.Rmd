---
title: "Linear Regression"
author: "Benjamin Khothsombath"
date: "8/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Regression

### Preparing the Data

First, I want to tackle linear regression. Simply put, given some numerical data, I want to be able to predict some output. For example, one can try to predict housing prices based on data such as square footage, date built, number of bedrooms, etc. For this example, I will be looking at the diamonds dataset in the `ggplot2` library. So first, we will load the dataset in, and make sure that we have loaded it properly using the `head()` function. This function loads the first 6(default) rows of a dataset.

```{r}
library(ggplot2)
data(diamonds)
head(diamonds)
```

Now that the data is loaded properly, we want to restructure it so that the categorical variables are split into columns for each category, and for each column, there is either a 1 or 0 depending on if the diamond falls into that category. So we can make a function that returns a dataframe, splitting the category into different columns, and we can then append them to the dataset, so let's make that function.

```{r}
split_category <- function(df, category) {
  levs <- df[[category]]
  m <- nrow(df)
  n <- length(levels(levs))
  new_df <- t(vapply(as.numeric(levs), function(num) {
    diag(n)[num, ]
  }, numeric(n)))
  colnames(new_df) <- levels(levs)
  new_df
}

# Copy dataset just in case
num_diamonds <- diamonds

for (category in c("cut", "color", "clarity")) {
  # Create New data frame to add
  new_df_to_add <- split_category(diamonds, category)
  
  # Add to data frame
  num_diamonds <- cbind(num_diamonds, new_df_to_add)
  
}

# Remove Categorical Columns
remove <- which(colnames(num_diamonds) %in% c("cut", "color", "clarity"))
num_diamonds <- num_diamonds[, -remove]
head(num_diamonds)
```

Now that we have the dataset in the form that we want, we can clean up our workspace a little bit.

```{r}
rm(new_df_to_add, remove)
```


We now want to split up the data into a training set and a test set. We'll make 80% of the data our training set and 20% our test set. First, we shuffle the data. We do this by supposing that there are $m$ observations(rows). Then, we shuffle the vector $(1, 2, ..., m)$ and use this to shuffle the rows of our dataset.

```{r}
set.seed(1234)
num_diamonds <- num_diamonds[sample(nrow(num_diamonds)), ]
```

Now, we need to split the data into our training set and our test set, which is easy.

```{r}
log_index <- seq_len(nrow(num_diamonds)) <= 0.8 * nrow(num_diamonds)
training_set <- num_diamonds[log_index, ]
test_set <- num_diamonds[!log_index, ]
```

And we can verify that we have done this properly by running these two commands:

```{r}
nrow(training_set)
nrow(test_set)
```

Finally, we want to separate the output variable (price) from the data set of both, then add $x_0 = 1$ to the columns of both sets. We will call our training data $X$.

```{r}
y <- training_set$price
test_outputs <- test_set$price
training_set <- training_set[, colnames(training_set) != "price"]
test_set <- test_set[, colnames(test_set) != "price"]
X <- cbind(rep(1, nrow(training_set)), training_set)
```


### Creating our Functions/Formulas

Before beginning, we want to use mean normalization to ensure that our gradient descent algorithm will converge much faster. Gradient descent will always converge to a global minimum with this cost function, but this is to ensure that it converges faster.

```{r}
meanNormalize <- function(X) {
  # X is a data frame with m rows and n + 1 columns
  # returns a list where new_X is the normalized data set
  # mu is the means of each column
  # sigma is the sds of each colun
  new_X <- apply(X[, 2:ncol(X)], 2, function(col) {
    mean <- mean(col)
    sd <- sd(col)
    (col - mean) / sd
  })
  new_X <- cbind(X[, 1], new_X)
  mu <- apply(X, 2, mean)
  sigma <- apply(X, 2, sd)
  l <- list(new_X, mu, sigma)
  names(l) <- c("X_norm", "mu", "sigma")
  l
}
```


Now that we have properly split up our data, we first need to make our linear model. A linear model $n$ with features(variables) $x_1, x_2, \dots, x_n$ has the form $$h(\theta_0, \theta_1, \theta_2, \dots, \theta_n) = \theta_0 + \theta_1x_1 + \dots + \theta_nx_n$$ In this case, the $x_i$'s are the data that we are given. We have this information, but we want to make a good linear model to predict the price of a diamond, so we need to find the best $\theta_i$'s to do so. First, we make a function that can predict the price of a diamond, assuming we already know the input parameters $\theta_0, \theta_1, \dots, \theta_n$ of a diamond and its features $x_1, x_2, \dots, x_n$. Using our linear model, we can make this function:

```{r}
predictPrices <- function(X, theta) {
  # X is a data frame with m rows and n + 1 columns
  # theta is a vector with n + 1 entries
  apply(X, 1, function(row) {
    sum(row * theta)
  })
}
```

Here, we simply added a 1 to the beginning of the vector $x$. Then, we use the vectorized multiplication function to multiply the vectors pairwise, where $x_0 = 1$. Then we take the sum, and this will give us our predicted price.

We now want a way to determine how good of a job our model is. That is, we want our model to make as few errors as possible, but how do we assess errors mathematically? Well, simply put, we can subtract the predicted price from the actual price to see how good it was for that particular diamond, then take the mean-squared errors to see how the model did in predicting all of the prices and sum them. So, given a vector $h$ of predicted prices and $y$ of actual prices, our formula for our cost function is $$J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$$ where $h_{\theta}(x^{(i)})$ is the predicted price of the ith diamond, $y^{(i)}$ is the actual price of the ith diamond, and $m$ is the number of diamonds, or what we would call *training examples*. We divide by $2m$ just so we can make it easier for later. Doing this has no effect besides rescaling the function (for example, if you divide a function $f(x)$ by 2, it still maintains its form, just shrunken down).

```{r}
costFunction <- function(X, y, theta) {
  if (nrow(X) != length(y)) {
    stop("Inputs are not of the same length.")
  }
  h <- predictPrices(X, theta)
  m <- nrow(X)
  # h and y are vectors of length m
  (1 / (2 * m)) * sum((h - y) ^2)
}
```

Before proceeding, it's best to perform what's known as "feature scaling" on our dataset. This is because we're going to try to minimize our cost function, and while we don't have to perform feature scaling, it speeds up our minimization process. We'll use mean normalization to do this. That is, for each variable $x_i$ with mean $\mu_i$ and range $s_i$, we will reaplce each $x_i$ with $x_i = \frac{x_i - \mu_i}{s_i}$.

Now we would like to implement **gradient descent**, which is the process by which we minimize $J(\theta)$ by adjusting the $\theta$'s little by little until they get close enough to the minimum of the cost function. It can be proven mathematically that the only miniumum of the cost function is the global minimum, so it cannot get stuck in a local optima. We perform the following computation until we are close enough to the minimum: $$\theta_j = \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$$ where $\alpha$ is some small number known as the learning rate. It can be proven mathematically that there exists some $\alpha_{thresh} \gt 0$ such that gradient descent will always converge for this particular cost function. However, a learning rate that is too large will cause the algorithm to diverge. That being said, we first begin our implementation of gradient descent by computing the derivative, which is $$\frac{\partial}{\partial \theta_j}J(\theta) = \frac{1}{m} \sum_{i = 1}^m (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$

With this, we can now make a function to perform gradient descent to find $\theta$'s that minimize the cost function.

```{r}
gradientDescent <- function(X, y, theta, alpha, max_iter) {
  m <- nrow(X)
  j_history <- numeric(0)
  for (i in seq(max_iter)) {
    h <- predictPrices(X, theta)
    diff <- h - y
    deriv <- apply(X, 2, function(col) {
      sum(diff * col)
    })
    theta <- theta - (deriv * (alpha / m))
    j_history[i] <- costFunction(X, y, theta)
  }
  list("j_history" = j_history, "theta" = theta)
}
```

### Making our Model

Now, we want to initalize our $\theta$'s, which we can do by choosing them randomly. We first define our variables $n$, the number of features and $m$, the number of training examples, and we'll also output the beginning of our datasets just to ensure we've done it correctly.

```{r}
n <- ncol(X) - 1
m <- nrow(X)
theta <- rep(0, n + 1)
head(X)
head(y)
theta
```

Finally, let's use mean normalization on X to scale it down, and begin training our model.

```{r, cache=TRUE}
X_norm <- meanNormalize(X)$X_norm
model <- gradientDescent(X_norm, y, theta, alpha = 0.25, max_iter = 1500)
```

### Results

I ran the model multiple times with different values for `alpha` and `max_iter` and found that these values work best. This value of alpha is large enough that gradient descent converges quickly, but not too large so that it diverges. 1500 iterations of gradient descent are more than enough to get close-enough values for `theta`. I've coded my gradient descent algorithm to keep track of how well it's doing in decreasing the cost function, which I've plotted the first 100 iterations for below.

```{r}
marks <- (0:10) * 1000000
labs <- c(0, paste(as.character(1:10), "M", sep = ""))
plot(x = 1:100, y = model$j_history[1:100], pch = 4, col = "blue",
     main = "J(theta) vs. Iteration", type = "l",
     ylab = expression(J(theta)), xlab = "Iteration",
     ylim = c(1000000, 10000000), yaxt = "n")
axis(2, at = marks, labels = labs)
```

As you can see, gradient descent works quickly for the first 20 or so values, but then inches slowly toward the minimum afterward. Now, we can test our model on our test set to see how well it did.

```{r}
X_test <- cbind(rep(1, nrow(test_set)), test_set)
results <- cbind(predictPrices(meanNormalize(X_test)[[1]], model$theta), test_outputs)
colnames(results) <- c("Predicted Price", "Actual Price")
head(results, n = 30)
```

And as you can see, our model does an okay job of predicting the price of a diamond. It can predict when a diamond is expensive and when it isn't, but it also has a bit of error. Interestingly, some predicted values turned out to be negative.

I'll graph the predicted prices vs the actual prices of the first 200 diamonds so we can visualize it a little better.

```{r}
plot(x = results[1:200, 1], y = results[1:200, 2], xlab = "Predicted Price", ylab = "Actual Price",
     main = "Predicted Price vs. Actual Price", pch = 4, col = "blue",
     xlim = c(-700, 22000), ylim = c(-700, 22000))
lines(x = c(0, 22000), y = c(0, 22000), col = "green", lty = 2)
```


